{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 15:16:03.653125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 15:16:03.768723: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:16:03.768740: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-06 15:16:03.791725: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-06 15:16:04.292494: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:16:04.292578: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:16:04.292585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import soundcard as sc\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.models import load_model\n",
    "# import test2\n",
    "# import make_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from sklearn.decomposition import PCA\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import librosa.display\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def record_and_save_clips(record_time):\n",
    "    output_path = r'E:/job/audio/recordings'\n",
    "    \n",
    "    headphones_device_id = \"Headphones (Realtek(R) Audio)\"\n",
    "   \n",
    "\n",
    "    try:\n",
    "            output_file_name = f\"{output_path}/out_{clip_count}.wav\"\n",
    "            samplerate = 16000\n",
    "            with sc.get_microphone(id=headphones_device_id, include_loopback=True).recorder(samplerate=samplerate) as mic:\n",
    "                \n",
    "                data = mic.record(numframes=samplerate * record_time)\n",
    "                sf.write(file=output_file_name, data=data[0:, 0], samplerate=samplerate)\n",
    "                print(\"Saved successfully \" + str(clip_count))\n",
    "                \n",
    "\n",
    "    except Exception as e:\n",
    "            print(\"An error occurred:\", str(e))\n",
    "#it will save 1 clip\n",
    "\n",
    "\n",
    "def extract_feature(base_path, clip_count, max_pad_length=86):\n",
    "    # Combine the base path and variable part to form the complete file path\n",
    "    file_name = f\"/home/sae/saevoice.wav\"\n",
    "    \n",
    "    try:\n",
    "            audio, sample_rate = librosa.load(file_name)\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "            pad_width = max_pad_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, max(0, pad_width))), mode='constant')\n",
    "    except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file_name)\n",
    "            return None\n",
    "    return mfccs\n",
    "#it will process the same clip into mfcc\n",
    "\n",
    "def predict(mfccs):\n",
    "      \n",
    "      h5_file_path = r'E:/job/audio/for2_mfcc.h5'\n",
    "      loaded_model = load_model(h5_file_path)\n",
    "      new_predictions=loaded_model.predict(mfccs)\n",
    "      # Load the existing CSV file with previous predictions\n",
    "      predict_path=r'E:/job/audio/predictions.csv'\n",
    "      previous_predictions_df = pd.read_csv(predict_path)\n",
    "      # Create a new DataFrame for the new predictions\n",
    "      new_predictions_df = pd.DataFrame(new_predictions, columns=['New_Prediction'])\n",
    "\n",
    "      # Concatenate the previous and new predictions horizontally (along columns)\n",
    "      combined_predictions_df = pd.concat([previous_predictions_df, new_predictions_df], axis=1)\n",
    "\n",
    "      # Save the combined DataFrame to the same CSV file\n",
    "      combined_predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "#it will predict the result and save it in a csv file \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m number_of_clips \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mEnter the number of clips to record: \u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m record_time \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter the duration of each clip (in seconds): \u001b[39m\u001b[39m\"\u001b[39m))   \n\u001b[1;32m      3\u001b[0m clip_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "number_of_clips = int(input(\"Enter the number of clips to record: \"))\n",
    "record_time = int(input(\"Enter the duration of each clip (in seconds): \"))   \n",
    "clip_count = 1\n",
    "#the function will run as long as each clip has not been predicted \n",
    "# input(\"Press enter to start recording....\")\n",
    "record_and_save_clips(record_time)\n",
    "#take 65 seconds only\n",
    "#and number of recordings as 1 right now we will add the while loop later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracted\n",
      "MFCC shape: (40, 2845)\n"
     ]
    }
   ],
   "source": [
    "mfcc_data=extract_feature(r'E:/job/audio/recordings',1)\n",
    "print(\"feature extracted\")\n",
    "print(\"MFCC shape:\", mfcc_data.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape: (40, 2845)\n"
     ]
    }
   ],
   "source": [
    "input_data=mfcc_data\n",
    "print(\"MFCC shape:\", mfcc_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_shape = (40, 2784)\n",
    "\n",
    "if mfcc_data.shape[1] > desired_shape[1]:\n",
    "    # Slice the data to fit the desired shape\n",
    "    truncated_data = mfcc_data[:, :desired_shape[1]]\n",
    "else:\n",
    "    print(\"Cannot truncate. Desired shape is larger or equal to the original shape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 2784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_shape = (32, 40, 87)\n",
    "\n",
    "# Check if the shapes are compatible\n",
    "if np.prod(truncated_data.shape) != np.prod(desired_shape):\n",
    "    print(\"Cannot reshape. Total number of elements must remain the same.\")\n",
    "else:\n",
    "    reshaped_data = truncated_data.reshape(desired_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 15:28:45.731814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.731947: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732032: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732197: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732278: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-10-06 15:28:45.732455: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-10-06 15:28:45.732829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded\n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "[[2.2873646e-03 9.9771255e-01]\n",
      " [1.0796006e-08 9.9999994e-01]\n",
      " [2.6849676e-08 9.9999994e-01]\n",
      " [2.1977413e-03 9.9780226e-01]\n",
      " [9.3579304e-01 6.4207025e-02]\n",
      " [8.5648757e-01 1.4351243e-01]\n",
      " [4.3285675e-02 9.5671433e-01]\n",
      " [9.8051274e-01 1.9487260e-02]\n",
      " [9.9994546e-01 5.4495289e-05]\n",
      " [2.2047525e-05 9.9997789e-01]\n",
      " [6.9536828e-03 9.9304634e-01]\n",
      " [7.7232534e-01 2.2767462e-01]\n",
      " [9.0649241e-01 9.3507491e-02]\n",
      " [1.9405162e-02 9.8059481e-01]\n",
      " [8.7992549e-03 9.9120063e-01]\n",
      " [4.9097589e-01 5.0902408e-01]\n",
      " [6.2045287e-02 9.3795460e-01]\n",
      " [6.2794872e-02 9.3720508e-01]\n",
      " [9.9007279e-01 9.9272626e-03]\n",
      " [1.7597207e-01 8.2402796e-01]\n",
      " [9.9986911e-01 1.3085250e-04]\n",
      " [9.6944338e-01 3.0556628e-02]\n",
      " [8.7548369e-01 1.2451623e-01]\n",
      " [2.6902657e-03 9.9730980e-01]\n",
      " [2.0903128e-05 9.9997908e-01]\n",
      " [5.9522954e-08 9.9999994e-01]\n",
      " [3.7529138e-05 9.9996239e-01]\n",
      " [5.0344585e-08 9.9999994e-01]\n",
      " [6.1512645e-07 9.9999934e-01]\n",
      " [1.6730425e-09 9.9999994e-01]\n",
      " [7.2956020e-01 2.7043983e-01]\n",
      " [1.1855330e-01 8.8144660e-01]]\n"
     ]
    }
   ],
   "source": [
    "h5_file_path = r'model1.h5'\n",
    "loaded_model = load_model(h5_file_path)\n",
    "print(\"model is loaded\")\n",
    "new_predictions=loaded_model.predict(reshaped_data)\n",
    "print(new_predictions)\n",
    "#0 is for fake\n",
    "#1 is for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "binary_predictions = [1 if prob[1] >= threshold else 0 for prob in new_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions= (new_predictions[:, 1] > 0.7).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print( binary_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "avg_prediction = sum(binary_predictions) / len(binary_predictions)\n",
    "print(avg_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18 | packaged by conda-forge | (main, Aug 30 2023, 03:49:32) \n[GCC 12.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2df73002193cefbfa9243a6efa2b74af79e3047145117f20857a75b11408ac59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
